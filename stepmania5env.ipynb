{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install gym\n",
    "# %pip install keras\n",
    "# %pip install keras-rl2\n",
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from matplotlib import pyplot as plt\n",
    "from mss import mss\n",
    "import numpy as np \n",
    "import time\n",
    "import pydirectinput\n",
    "import pygetwindow\n",
    "import cv2 as cv2\n",
    "import win32gui\n",
    "import gym as gym\n",
    "from image_analysis.take_screenshot import Screenshot\n",
    "from input_sending.input_sending import SendInput \n",
    "from pattern_recognition.pattern_recog import RecognizePattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepManiaEnv(Env):\n",
    "    \n",
    "    # Setup\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Defined action space\n",
    "        self.action_space = gym.spaces.Discrete(16)\n",
    "\n",
    "        # Observation Array\n",
    "        self.observation_space = Box(\n",
    "            low=0, \n",
    "            high=255, \n",
    "            shape=(1, 135, 100),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        # Define extraction parameters for the game\n",
    "        self.screenshot_helper = Screenshot()\n",
    "        self.input_sending_helper = SendInput()\n",
    "        self.pattern_recog_helper = RecognizePattern()\n",
    "        self.capture = mss()\n",
    "        self.steps = 0\n",
    "        self.previous_reward = 0\n",
    "        self.previous_input = \"0000\"\n",
    "        self.window_location = {'top': 35, 'left': 10, 'width': 410, 'height': 230}\n",
    "        self.game_location = {'top': 15, 'left': 20, 'width': 100, 'height': 185}\n",
    "        self.score_location = {'top': 215, 'left': 280, 'width': 100, 'height': 25}\n",
    "        self.done_location = {'top': 0, 'left': 0, 'width': 90, 'height':25}\n",
    "        self.past_arrows_location = {'top': 45, 'left': 50, 'width': 140, 'height': 2}\n",
    "        self.action_map = {\n",
    "            0:'a',\n",
    "            1:'s',\n",
    "            2:'w',\n",
    "            3:'d',\n",
    "        }\n",
    "        \n",
    "        # Adjust window position and size\n",
    "        win = pygetwindow.getWindowsWithTitle('StepMania')[1]\n",
    "        win.size = (450, 290)\n",
    "        win.moveTo(0, 0)\n",
    "\n",
    "    # One iteration of the environment\n",
    "    def step(self, action):\n",
    "        \n",
    "        action_array = self.decimal_to_binary(action)\n",
    "\n",
    "        # Manage input based on action parameter\n",
    "        for i in range(len(action_array)):\n",
    "            # If no change has occured, keep the status of the key\n",
    "            if action_array[i] == self.previous_input[i]:\n",
    "                continue\n",
    "            elif action_array[i] == \"1\":\n",
    "                self.input_sending_helper.holdKey(self.action_map[i])\n",
    "            elif action_array[i] == \"0\":\n",
    "                self.input_sending_helper.releaseKey(self.action_map[i])\n",
    "\n",
    "        # Save input from this step for the next step\n",
    "        self.previous_input = action_array\n",
    "       \n",
    "        # Take screenshot for done, observation and reward functions\n",
    "        screenshot = np.array(self.capture.grab(self.window_location))[:,:,:-1].astype(np.uint8)\n",
    "        downscaled_screenshot = self.screenshot_helper.downscaleImageBinary(screenshot, (225, 150), (1, 150, 225))\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Checking if the game is over\n",
    "        done = self.get_over(screenshot)\n",
    "        \n",
    "        # Get the next observation\n",
    "        new_observation = self.get_observation(downscaled_screenshot)\n",
    "\n",
    "        # Use score as reward\n",
    "        reward = self.get_reward(screenshot, action_array)\n",
    "        info = {}\n",
    "\n",
    "        return new_observation, reward, done, info\n",
    "\n",
    "    # Quits result screen and selects new song\n",
    "    def reset(self):\n",
    "\n",
    "        # Exit to menu, select new song and start\n",
    "        time.sleep(5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(6)\n",
    "        pydirectinput.press('d')\n",
    "        time.sleep(2)\n",
    "        pydirectinput.press('enter')\n",
    "\n",
    "        # Edge Case - 'Roulette' is selected\n",
    "        time.sleep(1.5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(3)\n",
    "        pydirectinput.press('enter')\n",
    "\n",
    "        # Reset variables\n",
    "        self.previous_reward = 0\n",
    "        self.steps = 0\n",
    "        self.previous_input = \"0000\"\n",
    "\n",
    "        # Take screenshot to pass to observation\n",
    "        screenshot = np.array(self.capture.grab(self.window_location))[:,:,:-1].astype(np.uint8)\n",
    "        downscaled_screenshot = self.screenshot_helper.downscaleImageBinary(screenshot, (225, 150), (1, 150, 225))\n",
    "\n",
    "        print(self.get_observation(downscaled_screenshot))\n",
    "\n",
    "        return self.get_observation(downscaled_screenshot)\n",
    "\n",
    "    # Returns image of gameplay\n",
    "    def get_observation(self, img):\n",
    "\n",
    "        # Crop gameplay part of the window screenshot\n",
    "        obs = img[:, self.game_location['top']:(self.game_location['top'] + self.game_location['height']), self.game_location['left']:(self.game_location['left'] + self.game_location['width'])]\n",
    "\n",
    "        return obs\n",
    "            \n",
    "\n",
    "    # Returns the current score as a reward\n",
    "    def get_reward(self, img, action):\n",
    "\n",
    "        # Crop score part of the window screenshot and top of the gameplay section\n",
    "        score_img = img[self.score_location['top']:(self.score_location['top'] + self.score_location['height']), self.score_location['left']:(self.score_location['left'] + self.score_location['width'])]\n",
    "        past_arrows_img = img[env.past_arrows_location['top']:(env.past_arrows_location['top'] + env.past_arrows_location['height']), env.past_arrows_location['left']:(env.past_arrows_location['left'] + env.past_arrows_location['width'])]\n",
    "\n",
    "        # If no input should have occured, give negative reward\n",
    "        if (self.pattern_recog_helper.input_expected(past_arrows_img, action) == False):\n",
    "            return -1\n",
    "\n",
    "        # If the score increased, give positive reward\n",
    "        new_reward = self.pattern_recog_helper.analyze_score(score_img)\n",
    "        if (new_reward > self.previous_reward):\n",
    "            # Set the current reward as the previous reward for the next iteration\n",
    "            self.previous_reward = new_reward\n",
    "            return 3\n",
    "            \n",
    "        # If the score didn't change, and no action has taken place, give a neutral reward\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Checks if the game is over\n",
    "    def get_over(self, img):\n",
    "\n",
    "        # Crop done part of the window screenshot\n",
    "        obs = img[self.done_location['top']:(self.done_location['top'] + self.done_location['height']), self.done_location['left']:(self.done_location['left'] + self.done_location['width'])]\n",
    "        \n",
    "        return self.pattern_recog_helper.analyze_results(obs)\n",
    "\n",
    "    # Translates action space integer into binary\n",
    "    # Each digit represents the condition of the key (from left to right)\n",
    "    # 0 = not pressed, 1 = pressed\n",
    "    def decimal_to_binary(self, number):\n",
    "        binary = \"\"\n",
    "\n",
    "        if number == 0:\n",
    "            return \"0000\"\n",
    "        \n",
    "        while number > 0:\n",
    "            remainder = number % 2\n",
    "            binary = str(remainder) + binary\n",
    "            number = number // 2\n",
    "\n",
    "        # Fill to always be 4 digits long\n",
    "        binary = binary.zfill(4)\n",
    "\n",
    "        return binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment started - Using random inputs')\n",
    "time.sleep(2)\n",
    "for episode in range(10):\n",
    "  done = False\n",
    "  start = time.perf_counter()\n",
    "  total_reward = 0\n",
    "  while not done:\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    total_reward += reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "  print(f'Total Reward for episode {episode} is {total_reward}')\n",
    "  print(f'Total steps during this episode: {env.steps}')\n",
    "  print(f'Total duration of this episode is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {env.steps / final_time} steps per second')\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the environment is valid\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function, that is called after every step\n",
    "# This is used to save the model in regular intervals\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "  def __init__(self, checking_freq, save_path, verbose=1):\n",
    "    super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "    self.checking_freq = checking_freq\n",
    "    self.save_path = save_path\n",
    "\n",
    "  def _init_callback(self):\n",
    "    if self.save_path is not None:\n",
    "      os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "  def _on_step(self):\n",
    "    if (self.num_timesteps % 500 == 0):\n",
    "      self.logger.dump(self.num_timesteps)\n",
    "    if self.n_calls % self.checking_freq == 0:\n",
    "      model_path = os.path.join(self.save_path, f'best_model_{self.n_calls}')\n",
    "      self.model.save(model_path)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './training/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(checking_freq=50_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap environment to monitor performance and training process\n",
    "env = gym.make(\"StepManiaEnv-v2\")\n",
    "env = Monitor(env, filename=\"./logs/stepmania-env-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "  'CnnPolicy',              \n",
    "  env,                      # Used environment\n",
    "  tensorboard_log=LOG_DIR,  # Log directory\n",
    "  verbose=1,                # Enables logging\n",
    "  learning_rate=0.0005,     # Learning rate of the optimizer used in training\n",
    "  buffer_size=120_000,      # Buffer size depending on amount of ram\n",
    "  learning_starts=1_000,    # Learning starts after 1000 steps\n",
    "  # device='cpu'              # Training on cpu or gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "model.learn(total_timesteps=1_000_000, callback=callback, log_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load past model \n",
    "model = DQN.load(r'training\\DQN13_best_model_1000000', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "model = DQN.load(\n",
    "  r'training\\DQN11_comparison_binary_long', \n",
    "  env=env, \n",
    "  tensorboard_log=\"./logs/DQN11_continued_training\",\n",
    ")\n",
    "\n",
    "model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=1_000_000, callback=callback, log_interval=1, reset_num_timesteps=False, tb_log_name=\"second_run\")\n",
    "\n",
    "model.save(r\"training\\DQN11_continued_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment started - Using Model')\n",
    "for episode in range(10):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  start = time.perf_counter()\n",
    "  total_reward = 0\n",
    "  while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(int(action))\n",
    "    total_reward += reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "  print(f'Total Reward for episode {episode} is {env.total_reward}')\n",
    "  print(f'Total steps during this episode: {env.steps}')\n",
    "  print(f'Total duration of this episode is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {env.steps / final_time} steps per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "results_plotter.plot_results(\n",
    "  [LOG_DIR], 10000, results_plotter.X_TIMESTEPS, \"StepManiaEnv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
