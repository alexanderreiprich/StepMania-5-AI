{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install gym\n",
    "# %pip install keras\n",
    "# %pip install keras-rl2\n",
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from matplotlib import pyplot as plt\n",
    "from mss import mss\n",
    "import numpy as np \n",
    "import time\n",
    "import pydirectinput\n",
    "import pygetwindow\n",
    "import cv2 as cv2\n",
    "import win32gui\n",
    "import gym as gym\n",
    "from image_analysis.take_screenshot import Screenshot\n",
    "from input_sending.input_sending import SendInput \n",
    "from pattern_recognition.pattern_recog import RecognizePattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepManiaEnv(Env):\n",
    "    \n",
    "    # Setup\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Defined action space\n",
    "        self.action_space = gym.spaces.Discrete(16)\n",
    "\n",
    "        # Observation Array\n",
    "        self.observation_space = Box(\n",
    "            low=0, \n",
    "            high=255, \n",
    "            shape=(1, 135, 100),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        # Define extraction parameters for the game\n",
    "        self.screenshot_helper = Screenshot()\n",
    "        self.input_sending_helper = SendInput()\n",
    "        self.pattern_recog_helper = RecognizePattern()\n",
    "        self.capture = mss()\n",
    "        self.steps = 0\n",
    "        self.previous_reward = 0\n",
    "        self.previous_observation = None\n",
    "        self.previous_input = \"0000\"\n",
    "        self.window_location = {'top': 35, 'left': 10, 'width': 410, 'height': 230}\n",
    "        self.game_location = {'top': 15, 'left': 20, 'width': 100, 'height': 185}\n",
    "        self.score_location = {'top': 215, 'left': 280, 'width': 100, 'height': 25}\n",
    "        self.done_location = {'top': 0, 'left': 0, 'width': 90, 'height':25}\n",
    "        self.past_arrows_location = {'top': 45, 'left': 50, 'width': 140, 'height': 2}\n",
    "        self.action_map = {\n",
    "            0:'a',\n",
    "            1:'s',\n",
    "            2:'w',\n",
    "            3:'d',\n",
    "        }\n",
    "        \n",
    "        # Adjust window position and size\n",
    "        win = pygetwindow.getWindowsWithTitle('StepMania')[1]\n",
    "        win.size = (450, 290)\n",
    "        win.moveTo(0, 0)\n",
    "\n",
    "    # One iteration of the environment\n",
    "    def step(self, action):\n",
    "        \n",
    "        action_array = self.decimal_to_binary(action)\n",
    "\n",
    "        # Manage input based on action parameter\n",
    "        for i in range(len(action_array)):\n",
    "            # If no change has occured, keep the status of the key\n",
    "            if action_array[i] == self.previous_input[i]:\n",
    "                continue\n",
    "            elif action_array[i] == \"1\":\n",
    "                self.input_sending_helper.holdKey(self.action_map[i])\n",
    "            elif action_array[i] == \"0\":\n",
    "                self.input_sending_helper.releaseKey(self.action_map[i])\n",
    "\n",
    "        # Save input from this step for the next step\n",
    "        print(self.previous_input, action_array)\n",
    "        self.previous_input = action_array\n",
    "       \n",
    "        # Take screenshot for done, observation and reward functions\n",
    "        screenshot = np.array(self.capture.grab(self.window_location))[:,:,:-1].astype(np.uint8)\n",
    "        downscaled_screenshot = self.screenshot_helper.downscaleImageBinary(screenshot, (225, 150), (1, 150, 225))\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Checking if the game is over\n",
    "        done = self.get_over(screenshot)\n",
    "        \n",
    "        # Get the next observation and save it in the environment for the next step\n",
    "        new_observation = self.get_observation(downscaled_screenshot)\n",
    "        self.previous_observation = new_observation\n",
    "\n",
    "        # Use score as reward\n",
    "        reward = self.get_reward(screenshot, action_array)\n",
    "        info = {}\n",
    "\n",
    "        return new_observation, reward, done, info\n",
    "\n",
    "    # Quits result screen and selects new song\n",
    "    def reset(self):\n",
    "\n",
    "        # Exit to menu, select new song and start\n",
    "        time.sleep(5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(6)\n",
    "        pydirectinput.press('d')\n",
    "        time.sleep(2)\n",
    "        pydirectinput.press('enter')\n",
    "\n",
    "        # Edge Case - 'Roulette' is selected\n",
    "        time.sleep(1.5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(3)\n",
    "        pydirectinput.press('enter')\n",
    "\n",
    "        # Reset variables\n",
    "        self.previous_reward = 0\n",
    "        self.previous_observation = None\n",
    "        self.steps = 0\n",
    "        self.previous_input = \"0000\"\n",
    "\n",
    "        # Take screenshot to pass to observation\n",
    "        screenshot = np.array(self.capture.grab(self.window_location))[:,:,:-1].astype(np.uint8)\n",
    "        downscaled_screenshot = self.screenshot_helper.downscaleImageBinary(screenshot, (225, 150), (1, 150, 225))\n",
    "        \n",
    "        return self.get_observation(downscaled_screenshot)\n",
    "\n",
    "    # Returns image of gameplay\n",
    "    def get_observation(self, img):\n",
    "\n",
    "        # Crop gameplay part of the window screenshot\n",
    "        obs = img[:, self.game_location['top']:(self.game_location['top'] + self.game_location['height']), self.game_location['left']:(self.game_location['left'] + self.game_location['width'])]\n",
    "\n",
    "        # Create an image containing the differences between this and the previous observation\n",
    "        # If no previous observation exists, return the initial observation\n",
    "        if (self.previous_observation is None):\n",
    "            return obs\n",
    "        else:\n",
    "            diff = cv2.absdiff(self.previous_observation, obs)\n",
    "            return diff\n",
    "            \n",
    "\n",
    "    # Returns the current score as a reward\n",
    "    def get_reward(self, img, action):\n",
    "\n",
    "        # Crop score part of the window screenshot and top of the gameplay section\n",
    "        score_img = img[self.score_location['top']:(self.score_location['top'] + self.score_location['height']), self.score_location['left']:(self.score_location['left'] + self.score_location['width'])]\n",
    "        past_arrows_img = img[env.past_arrows_location['top']:(env.past_arrows_location['top'] + env.past_arrows_location['height']), env.past_arrows_location['left']:(env.past_arrows_location['left'] + env.past_arrows_location['width'])]\n",
    "\n",
    "        # If no input should have occured, give negative reward\n",
    "        if (self.pattern_recog_helper.input_expected(past_arrows_img, action) == False):\n",
    "            return -1\n",
    "\n",
    "        # If the score increased, give positive reward\n",
    "        new_reward = self.pattern_recog_helper.analyze_score(score_img)\n",
    "        if (new_reward > self.previous_reward):\n",
    "            # Set the current reward as the previous reward for the next iteration\n",
    "            self.previous_reward = new_reward\n",
    "            return 3\n",
    "            \n",
    "        # If the score didn't change, and no action has taken place, give a neutral reward\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Checks if the game is over\n",
    "    def get_over(self, img):\n",
    "\n",
    "        # Crop done part of the window screenshot\n",
    "        obs = img[self.done_location['top']:(self.done_location['top'] + self.done_location['height']), self.done_location['left']:(self.done_location['left'] + self.done_location['width'])]\n",
    "        \n",
    "        return self.pattern_recog_helper.analyze_results(obs)\n",
    "\n",
    "    # Translates action space integer into binary\n",
    "    # Each digit represents the condition of the key (from left to right)\n",
    "    # 0 = not pressed, 1 = pressed\n",
    "    def decimal_to_binary(self, number):\n",
    "        binary = \"\"\n",
    "\n",
    "        if number == 0:\n",
    "            return \"0000\"\n",
    "        \n",
    "        while number > 0:\n",
    "            remainder = number % 2\n",
    "            binary = str(remainder) + binary\n",
    "            number = number // 2\n",
    "\n",
    "        # Fill to always be 4 digits long\n",
    "        binary = binary.zfill(4)\n",
    "\n",
    "        return binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment started - Using random inputs\n",
      "hold\n",
      "hold\n",
      "continue\n",
      "hold\n",
      "0000 1101\n",
      "continue\n",
      "release\n",
      "continue\n",
      "release\n",
      "1101 1000\n",
      "release\n",
      "hold\n",
      "continue\n",
      "hold\n",
      "1000 0101\n",
      "hold\n",
      "release\n",
      "hold\n",
      "continue\n",
      "0101 1011\n",
      "continue\n",
      "continue\n",
      "release\n",
      "release\n",
      "1011 1000\n",
      "release\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "1000 0000\n",
      "hold\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "0000 1100\n",
      "release\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "1100 0110\n",
      "hold\n",
      "release\n",
      "continue\n",
      "hold\n",
      "0110 1011\n",
      "release\n",
      "continue\n",
      "release\n",
      "continue\n",
      "1011 0001\n",
      "continue\n",
      "hold\n",
      "hold\n",
      "continue\n",
      "0001 0111\n",
      "continue\n",
      "release\n",
      "continue\n",
      "continue\n",
      "0111 0011\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "0011 0111\n",
      "hold\n",
      "release\n",
      "continue\n",
      "release\n",
      "0111 1010\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "hold\n",
      "1010 1111\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "release\n",
      "1111 1110\n",
      "continue\n",
      "continue\n",
      "release\n",
      "hold\n",
      "1110 1101\n",
      "continue\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "1101 1111\n",
      "release\n",
      "continue\n",
      "release\n",
      "continue\n",
      "1111 0101\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "0101 1101\n",
      "continue\n",
      "release\n",
      "hold\n",
      "continue\n",
      "1101 1011\n",
      "release\n",
      "hold\n",
      "continue\n",
      "release\n",
      "1011 0110\n",
      "continue\n",
      "release\n",
      "release\n",
      "hold\n",
      "0110 0001\n",
      "hold\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "0001 1011\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "release\n",
      "1011 1110\n",
      "release\n",
      "continue\n",
      "release\n",
      "hold\n",
      "1110 0101\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "0101 1101\n",
      "continue\n",
      "release\n",
      "hold\n",
      "release\n",
      "1101 1010\n",
      "release\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "1010 0110\n",
      "continue\n",
      "release\n",
      "release\n",
      "continue\n",
      "0110 0000\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "hold\n",
      "0000 1001\n",
      "continue\n",
      "hold\n",
      "hold\n",
      "release\n",
      "1001 1110\n",
      "release\n",
      "release\n",
      "continue\n",
      "continue\n",
      "1110 0010\n",
      "hold\n",
      "hold\n",
      "continue\n",
      "hold\n",
      "0010 1111\n",
      "release\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "1111 0111\n",
      "continue\n",
      "release\n",
      "continue\n",
      "continue\n",
      "0111 0011\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "release\n",
      "0011 0010\n",
      "hold\n",
      "continue\n",
      "release\n",
      "hold\n",
      "0010 1001\n",
      "release\n",
      "hold\n",
      "hold\n",
      "release\n",
      "1001 0110\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "hold\n",
      "0110 1111\n",
      "continue\n",
      "release\n",
      "continue\n",
      "continue\n",
      "1111 1011\n",
      "release\n",
      "hold\n",
      "release\n",
      "continue\n",
      "1011 0101\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "release\n",
      "0101 0100\n",
      "hold\n",
      "continue\n",
      "continue\n",
      "hold\n",
      "0100 1101\n",
      "release\n",
      "continue\n",
      "hold\n",
      "continue\n",
      "1101 0111\n",
      "continue\n",
      "release\n",
      "release\n",
      "continue\n",
      "0111 0001\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "0001 0001\n",
      "continue\n",
      "hold\n",
      "hold\n",
      "release\n",
      "0001 0110\n",
      "hold\n",
      "release\n",
      "continue\n",
      "continue\n",
      "0110 1010\n",
      "release\n",
      "hold\n",
      "release\n",
      "hold\n",
      "1010 0101\n",
      "continue\n",
      "release\n",
      "continue\n",
      "continue\n",
      "0101 0001\n",
      "hold\n",
      "hold\n",
      "continue\n",
      "release\n",
      "0001 1100\n",
      "release\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "1100 0100\n",
      "hold\n",
      "release\n",
      "hold\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m----> 8\u001b[0m   obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[0;32m      9\u001b[0m   total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     11\u001b[0m stop \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "Cell \u001b[1;32mIn[51], line 57\u001b[0m, in \u001b[0;36mStepManiaEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39melif\u001b[39;00m action_array[i] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mhold\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_sending_helper\u001b[39m.\u001b[39;49mholdKey(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_map[i])\n\u001b[0;32m     58\u001b[0m \u001b[39melif\u001b[39;00m action_array[i] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrelease\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mg:\\#Thesis\\ThesisRepo\\input_sending\\input_sending.py:6\u001b[0m, in \u001b[0;36mSendInput.holdKey\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mholdKey\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m----> 6\u001b[0m   pydirectinput\u001b[39m.\u001b[39;49mkeyDown(key)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Environment started - Using random inputs')\n",
    "time.sleep(2)\n",
    "for episode in range(10):\n",
    "  done = False\n",
    "  start = time.perf_counter()\n",
    "  total_reward = 0\n",
    "  while not done:\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    total_reward += reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "  print(f'Total Reward for episode {episode} is {total_reward}')\n",
    "  print(f'Total steps during this episode: {env.steps}')\n",
    "  print(f'Total duration of this episode is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {env.steps / final_time} steps per second')\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the environment is valid\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function, that is called after every step\n",
    "# This is used to save the model in regular intervals\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "  def __init__(self, checking_freq, save_path, verbose=1):\n",
    "    super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "    self.checking_freq = checking_freq\n",
    "    self.save_path = save_path\n",
    "\n",
    "  def _init_callback(self):\n",
    "    if self.save_path is not None:\n",
    "      os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "  def _on_step(self):\n",
    "    if (self.num_timesteps % 500 == 0):\n",
    "      self.logger.dump(self.num_timesteps)\n",
    "    if self.n_calls % self.checking_freq == 0:\n",
    "      model_path = os.path.join(self.save_path, f'best_model_{self.n_calls}')\n",
    "      self.model.save(model_path)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './training/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(checking_freq=50_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Wrap environment to monitor performance and training process\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mStepManiaEnv-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m env \u001b[39m=\u001b[39m Monitor(env, filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./logs/stepmania-env-v2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\registration.py:235\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39mmake(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\registration.py:129\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mMaking new env: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, path)\n\u001b[0;32m    128\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec(path)\n\u001b[1;32m--> 129\u001b[0m env \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39mmake(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\registration.py:89\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentry_point)\n\u001b[0;32m     90\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m     92\u001b[0m \u001b[39m# Make the environment aware of which spec it came from.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\registration.py:27\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name):\n\u001b[0;32m     26\u001b[0m     mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[0;32m     28\u001b[0m     fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpendulum\u001b[39;00m \u001b[39mimport\u001b[39;00m PendulumEnv\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39macrobot\u001b[39;00m \u001b[39mimport\u001b[39;00m AcrobotEnv\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstepmania5env\u001b[39;00m \u001b[39mimport\u001b[39;00m StepManiaEnv\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstepmania5env2\u001b[39;00m \u001b[39mimport\u001b[39;00m StepManiaEnv\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\stepmania5env.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStepManiaEnv\u001b[39;00m(Env):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[39m# Setup\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "# Wrap environment to monitor performance and training process\n",
    "env = gym.make(\"StepManiaEnv-v2\")\n",
    "env = Monitor(env, filename=\"./logs/stepmania-env-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gym.spaces.discrete.Discrete'>,) as action spaces but MultiDiscrete([2 2 2 2]) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m DQN(\n\u001b[0;32m      2\u001b[0m   \u001b[39m'\u001b[39;49m\u001b[39mCnnPolicy\u001b[39;49m\u001b[39m'\u001b[39;49m,              \n\u001b[0;32m      3\u001b[0m   env,                      \u001b[39m# Used environment\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m   tensorboard_log\u001b[39m=\u001b[39;49mLOG_DIR,  \u001b[39m# Log directory\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m   verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,                \u001b[39m# Enables logging\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m   learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.0005\u001b[39;49m,     \u001b[39m# Learning rate of the optimizer used in training\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m   buffer_size\u001b[39m=\u001b[39;49m\u001b[39m120_000\u001b[39;49m,      \u001b[39m# Buffer size depending on amount of ram\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m   learning_starts\u001b[39m=\u001b[39;49m\u001b[39m1_000\u001b[39;49m,    \u001b[39m# Learning starts after 1000 steps\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m   \u001b[39m# device='cpu'              # Training on cpu or gpu\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:99\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     73\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[DQNPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    100\u001b[0m         policy,\n\u001b[0;32m    101\u001b[0m         env,\n\u001b[0;32m    102\u001b[0m         learning_rate,\n\u001b[0;32m    103\u001b[0m         buffer_size,\n\u001b[0;32m    104\u001b[0m         learning_starts,\n\u001b[0;32m    105\u001b[0m         batch_size,\n\u001b[0;32m    106\u001b[0m         tau,\n\u001b[0;32m    107\u001b[0m         gamma,\n\u001b[0;32m    108\u001b[0m         train_freq,\n\u001b[0;32m    109\u001b[0m         gradient_steps,\n\u001b[0;32m    110\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# No action noise\u001b[39;49;00m\n\u001b[0;32m    111\u001b[0m         replay_buffer_class\u001b[39m=\u001b[39;49mreplay_buffer_class,\n\u001b[0;32m    112\u001b[0m         replay_buffer_kwargs\u001b[39m=\u001b[39;49mreplay_buffer_kwargs,\n\u001b[0;32m    113\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    114\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    115\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    116\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    117\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    118\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    119\u001b[0m         sde_support\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    120\u001b[0m         optimize_memory_usage\u001b[39m=\u001b[39;49moptimize_memory_usage,\n\u001b[0;32m    121\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(spaces\u001b[39m.\u001b[39;49mDiscrete,),\n\u001b[0;32m    122\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_initial_eps \u001b[39m=\u001b[39m exploration_initial_eps\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_final_eps \u001b[39m=\u001b[39m exploration_final_eps\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:108\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, use_sde_at_warmup, sde_support, supported_action_spaces)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     79\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     80\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[BasePolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m     supported_action_spaces: Optional[Tuple[spaces\u001b[39m.\u001b[39mSpace, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m ):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    109\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    110\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m    111\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    112\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    113\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    114\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    115\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    116\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    117\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49msupport_multi_env,\n\u001b[0;32m    118\u001b[0m         monitor_wrapper\u001b[39m=\u001b[39;49mmonitor_wrapper,\n\u001b[0;32m    119\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    120\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m    121\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m    122\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m=\u001b[39m buffer_size\n\u001b[0;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:172\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[0;32m    173\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    178\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gym.spaces.discrete.Discrete'>,) as action spaces but MultiDiscrete([2 2 2 2]) was provided"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "  'CnnPolicy',              \n",
    "  env,                      # Used environment\n",
    "  tensorboard_log=LOG_DIR,  # Log directory\n",
    "  verbose=1,                # Enables logging\n",
    "  learning_rate=0.0005,     # Learning rate of the optimizer used in training\n",
    "  buffer_size=120_000,      # Buffer size depending on amount of ram\n",
    "  learning_starts=1_000,    # Learning starts after 1000 steps\n",
    "  # device='cpu'              # Training on cpu or gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "model.learn(total_timesteps=1_000_000, callback=callback, log_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Load past model \n",
    "model = DQN.load(r'training\\DQN13_best_model_1000000', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "model = DQN.load(\n",
    "  r'training\\DQN11_comparison_binary_long', \n",
    "  env=env, \n",
    "  tensorboard_log=\"./logs/DQN11_continued_training\",\n",
    ")\n",
    "\n",
    "model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=1_000_000, callback=callback, log_interval=1, reset_num_timesteps=False, tb_log_name=\"second_run\")\n",
    "\n",
    "model.save(r\"training\\DQN11_continued_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment started - Using Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m      8\u001b[0m   action, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs)\n\u001b[1;32m----> 9\u001b[0m   obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(\u001b[39mint\u001b[39;49m(action))\n\u001b[0;32m     10\u001b[0m   total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     12\u001b[0m stop \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "Cell \u001b[1;32mIn[10], line 55\u001b[0m, in \u001b[0;36mStepManiaEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_held_buttons[\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_held_buttons)[action \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_sending_helper\u001b[39m.\u001b[39;49mholdKey(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_map[action])\n\u001b[0;32m     57\u001b[0m \u001b[39m# Take screenshot for done, observation and reward functions\u001b[39;00m\n\u001b[0;32m     58\u001b[0m screenshot \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture\u001b[39m.\u001b[39mgrab(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_location))[:,:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n",
      "File \u001b[1;32mg:\\#Thesis\\ThesisRepo\\input_sending\\input_sending.py:6\u001b[0m, in \u001b[0;36mSendInput.holdKey\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mholdKey\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m----> 6\u001b[0m   pydirectinput\u001b[39m.\u001b[39;49mkeyDown(key)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m _handlePause(funcArgs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Environment started - Using Model')\n",
    "for episode in range(10):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  start = time.perf_counter()\n",
    "  total_reward = 0\n",
    "  while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(int(action))\n",
    "    total_reward += reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "  print(f'Total Reward for episode {episode} is {env.total_reward}')\n",
    "  print(f'Total steps during this episode: {env.steps}')\n",
    "  print(f'Total duration of this episode is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {env.steps / final_time} steps per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(0, dtype=int64), None)\n"
     ]
    }
   ],
   "source": [
    "screenshot = np.array(env.capture.grab(env.window_location))[:,:,:-1].astype(np.uint8)\n",
    "downscaled_screenshot = env.screenshot_helper.downscaleImage(screenshot, (225, 150), (1, 150, 225))\n",
    "obs = env.get_observation(downscaled_screenshot)\n",
    "print(model.predict(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "results_plotter.plot_results(\n",
    "  [LOG_DIR], 10000, results_plotter.X_TIMESTEPS, \"StepManiaEnv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
