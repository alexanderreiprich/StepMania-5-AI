{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install gym\n",
    "# %pip install keras\n",
    "# %pip install keras-rl2\n",
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from matplotlib import pyplot as plt\n",
    "from mss import mss\n",
    "import numpy as np \n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "import pydirectinput\n",
    "import pygetwindow\n",
    "import cv2\n",
    "import win32gui\n",
    "import pytesseract\n",
    "from image_analysis_test.take_screenshot import Screenshot\n",
    "from input_sending_test.input_sending import SendInput \n",
    "from number_recognition_test.number_recog import RecognizeText \n",
    "from pattern_recognition_test.pattern_recog import RecognizePattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepManiaEnv(Env):\n",
    "    # Setup\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "        # Observation Array\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255, shape=(1, 170, 100), dtype=np.uint8)\n",
    "        \n",
    "        #shape=(1, 495, 290)\n",
    "\n",
    "        # Define extraction parameters for the game\n",
    "        self.screenshot_helper = Screenshot()\n",
    "        self.text_recog_helper = RecognizeText()\n",
    "        self.input_sending_helper = SendInput()\n",
    "        self.capture = mss()\n",
    "        self.game_location = {'top': 35, 'left': 110, 'width': 290, 'height': 495}\n",
    "        self.combo_location = {'top': 450, 'left': 570, 'width': 285, 'height': 30}\n",
    "        #'top': 450, 'left': 590, 'width': 285, 'height': 30\n",
    "        self.done_location = {'top': 40, 'left': 25, 'width': 150, 'height':30}\n",
    "        pytesseract.pytesseract.tesseract_cmd = r'G:\\Programme\\Tesseract\\tesseract.exe'\n",
    "\n",
    "    # What is called to do something in the game\n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        action_map = {\n",
    "            0:'no_op',\n",
    "            1:'a',\n",
    "            2:'d',\n",
    "            3:'w',\n",
    "            4:'s',\n",
    "        }\n",
    "\n",
    "        # TODO: Does this work with held notes?\n",
    "        # TODO: Change action from single input to array with inputs\n",
    "        # for action in actions:\n",
    "        if action != 0:\n",
    "            self.input_sending_helper.tapKey(action_map[action])\n",
    "            #pydirectinput.press(action_map[action])\n",
    "        \n",
    "        # Checking if the game is over\n",
    "        done = self.get_over()\n",
    "        # Get the next observation\n",
    "        new_observation = self.get_observation()\n",
    "        # Use score as reward\n",
    "        reward = self.get_reward()\n",
    "        info = {}\n",
    "\n",
    "        return new_observation, reward, done, info\n",
    "\n",
    "    # Restart the game\n",
    "    def reset(self):\n",
    "        time.sleep(5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(2)\n",
    "        pydirectinput.press('d')\n",
    "        pydirectinput.press('enter')\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Get the part of observation of the game that we want\n",
    "    def get_observation(self):\n",
    "        # Use Helper class to get screenshot\n",
    "        stepWindow = pygetwindow.getWindowsWithTitle('StepMania')\n",
    "        hwnd = stepWindow[0]._hWnd\n",
    "        bbox = win32gui.GetWindowRect(hwnd)\n",
    "\n",
    "        raw = np.array(self.capture.grab(self.game_location))[:,:,:-1].astype(np.uint8)\n",
    "        channel = self.screenshot_helper.downscaleImage(raw, (100, 170), (1, 170, 100))\n",
    "\n",
    "        return channel\n",
    "\n",
    "    # Get the current score as a reward\n",
    "    def get_reward(self):\n",
    "        stepWindow = pygetwindow.getWindowsWithTitle('StepMania')\n",
    "        hwnd = stepWindow[0]._hWnd\n",
    "        bbox = win32gui.GetWindowRect(hwnd)\n",
    "        \n",
    "        reward_img = np.array(self.capture.grab(self.combo_location))[:,:,:-1].astype(np.uint8)\n",
    "        channel = self.screenshot_helper.downscaleImage(reward_img, (285, 30), (1, 30, 285))\n",
    "        reward = env.text_recog_helper.get_number_from_image(channel[0])\n",
    "        return reward\n",
    "\n",
    "    # Get if the game is over\n",
    "    def get_over(self):\n",
    "        # Use Helper class to get screenshot\n",
    "        stepWindow = pygetwindow.getWindowsWithTitle('StepMania')\n",
    "        hwnd = stepWindow[0]._hWnd\n",
    "        bbox = win32gui.GetWindowRect(hwnd)\n",
    "\n",
    "        done_capture = np.array(self.capture.grab(self.done_location))[:,:,::-1].astype(np.uint8)\n",
    "        gray = cv2.cvtColor(done_capture, cv2.COLOR_BGR2GRAY)\n",
    "        # Valid done text\n",
    "        done_strings = ['Your Results']\n",
    "        return self.text_recog_helper.is_text_in_image(gray, done_strings, 12)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 2715\n",
      "Total screenshots for episode 0 is 167\n",
      "Total duration of episode 0 is 92.4896 seconds\n",
      "This equals an average of 1.805609124962938 images per second\n",
      "Total Reward for episode 1 is 8000\n",
      "Total screenshots for episode 1 is 35\n",
      "Total duration of episode 1 is 19.9062 seconds\n",
      "This equals an average of 1.7582484357215307 images per second\n",
      "Total Reward for episode 2 is 135\n",
      "Total screenshots for episode 2 is 191\n",
      "Total duration of episode 2 is 102.4751 seconds\n",
      "This equals an average of 1.8638681978348468 images per second\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m   obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m      3\u001b[0m   done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m   final_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mStepManiaEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m5\u001b[39m)\n\u001b[0;32m     55\u001b[0m pydirectinput\u001b[39m.\u001b[39mpress(\u001b[39m'\u001b[39m\u001b[39menter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     57\u001b[0m pydirectinput\u001b[39m.\u001b[39mpress(\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m pydirectinput\u001b[39m.\u001b[39mpress(\u001b[39m'\u001b[39m\u001b[39menter\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  final_reward = 0\n",
    "  amount_of_screenshots = 0\n",
    "  start = time.perf_counter()\n",
    "  while not done:\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    amount_of_screenshots += 1\n",
    "    if final_reward < reward:\n",
    "      final_reward = reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "  print(f'Total Reward for episode {episode} is {final_reward}')\n",
    "  print(f'Total screenshots for episode {episode} is {amount_of_screenshots}')\n",
    "  print(f'Total duration of episode {episode} is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {amount_of_screenshots / final_time} images per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the environment is valid\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "  def __init__(self, checking_freq, save_path, verbose=1):\n",
    "    super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "    self.checking_freq = checking_freq\n",
    "    self.save_path = save_path\n",
    "\n",
    "  def _init_callback(self):\n",
    "    if self.save_path is not None:\n",
    "      os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "  def _on_step(self):\n",
    "    if self.n_calls % self.checking_freq == 0:\n",
    "      model_path = os.path.join(self.save_path, f'best_model_{self.n_calls}')\n",
    "      self.model.save(model_path)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './training/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(checking_freq=1_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "  'CnnPolicy',              \n",
    "  env,                      # Used environment\n",
    "  tensorboard_log=LOG_DIR,  # Log directory\n",
    "  verbose=1,                # Enables logging\n",
    "  buffer_size=120_000,      # Buffer size depending on amount of ram\n",
    "  learning_starts=1_000,    # Learning starts after 1000 steps\n",
    "  #device='cpu'             # Training on cpu or gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_4\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 3e+08    |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 4        |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1_000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:269\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    262\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    270\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    271\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    272\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    273\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    274\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    275\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    276\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    308\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    310\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 311\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    313\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    314\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    315\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    316\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    317\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:543\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    540\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    542\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    546\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:60\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m         \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n\u001b[1;32m---> 60\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf(), np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews), np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones), deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:84\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected you to pass keyword argument \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m into reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_reset_info[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m---> 84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[10], line 55\u001b[0m, in \u001b[0;36mStepManiaEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m5\u001b[39m)\n\u001b[0;32m     54\u001b[0m pydirectinput\u001b[39m.\u001b[39mpress(\u001b[39m'\u001b[39m\u001b[39menter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     56\u001b[0m pydirectinput\u001b[39m.\u001b[39mpress(\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m pydirectinput\u001b[39m.\u001b[39mpress(\u001b[39m'\u001b[39m\u001b[39menter\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "model.learn(total_timesteps=1_000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load past model \n",
    "# model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  final_reward = 0\n",
    "  while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(int(action))\n",
    "    if final_reward < reward:\n",
    "      final_reward = reward\n",
    "\n",
    "  print(f'Total Reward for episode {episode} is {final_reward}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
