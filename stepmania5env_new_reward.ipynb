{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install gym\n",
    "# %pip install keras\n",
    "# %pip install keras-rl2\n",
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from matplotlib import pyplot as plt\n",
    "from mss import mss\n",
    "import numpy as np \n",
    "import time\n",
    "import pydirectinput\n",
    "import pygetwindow\n",
    "import cv2\n",
    "import win32gui\n",
    "import gym as gym\n",
    "from image_analysis.take_screenshot import Screenshot\n",
    "from input_sending.input_sending import SendInput \n",
    "from pattern_recognition.pattern_recog import RecognizePattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepManiaEnv(Env):\n",
    "    \n",
    "    # Setup\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Defined action space\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "        # Observation Array\n",
    "        self.observation_space = Box(\n",
    "            low=0, \n",
    "            high=255, \n",
    "            shape=(1, 135, 100),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        # Define extraction parameters for the game\n",
    "        self.screenshot_helper = Screenshot()\n",
    "        self.input_sending_helper = SendInput()\n",
    "        self.pattern_recog_helper = RecognizePattern()\n",
    "        self.capture = mss()\n",
    "        self.steps = 0\n",
    "        self.previous_reward = 0\n",
    "        self.window_location = {'top': 35, 'left': 10, 'width': 410, 'height': 230}\n",
    "        self.game_location = {'top': 15, 'left': 20, 'width': 100, 'height': 185}\n",
    "        self.score_location = {'top': 215, 'left': 280, 'width': 100, 'height': 25}\n",
    "        self.done_location = {'top': 0, 'left': 0, 'width': 90, 'height':25}\n",
    "        self.past_arrows_location = {'top': 45, 'left': 50, 'width': 140, 'height': 2}\n",
    "        self.cur_held_buttons = {'a': False, 's': False, 'w': False, 'd': False}\n",
    "        self.action_map = {\n",
    "            0:'no_op',\n",
    "            1:'a',\n",
    "            2:'s',\n",
    "            3:'w',\n",
    "            4:'d',\n",
    "        }\n",
    "        \n",
    "        # Adjust window position and size\n",
    "        win = pygetwindow.getWindowsWithTitle('StepMania')[1]\n",
    "        win.size = (450, 290)\n",
    "        win.moveTo(0, 0)\n",
    "\n",
    "    # One iteration of the environment\n",
    "    def step(self, action):\n",
    "\n",
    "        # Manage and send input based on action parameter\n",
    "        if action != 0:\n",
    "            if (list(self.cur_held_buttons.values())[action - 1]):\n",
    "                self.cur_held_buttons[list(self.cur_held_buttons)[action - 1]] = False\n",
    "                self.input_sending_helper.releaseKey(self.action_map[action])\n",
    "            else:\n",
    "                self.cur_held_buttons[list(self.cur_held_buttons)[action - 1]] = True\n",
    "                self.input_sending_helper.holdKey(self.action_map[action])\n",
    "        \n",
    "        \n",
    "        # Take screenshot for done, observation and reward functions\n",
    "        screenshot = np.array(self.capture.grab(self.window_location))[:,:,:-1].astype(np.uint8)\n",
    "        downscaled_screenshot = self.screenshot_helper.downscaleImage(screenshot, (225, 150), (1, 150, 225))\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Checking if the game is over\n",
    "        done = self.get_over(screenshot)\n",
    "        \n",
    "        # Get the next observation\n",
    "        new_observation = self.get_observation(downscaled_screenshot)\n",
    "        \n",
    "        # Use score as reward\n",
    "        reward = self.get_reward(screenshot, action)\n",
    "        info = {}\n",
    "\n",
    "        return new_observation, reward, done, info\n",
    "\n",
    "    # Quits result screen and selects new song\n",
    "    def reset(self):\n",
    "        \n",
    "        # Exit to menu, select new song and start\n",
    "        time.sleep(5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(6)\n",
    "        pydirectinput.press('d')\n",
    "        time.sleep(2)\n",
    "        pydirectinput.press('enter')\n",
    "\n",
    "        # Edge Case - 'Roulette' is selected\n",
    "        time.sleep(1.5)\n",
    "        pydirectinput.press('enter')\n",
    "        time.sleep(3)\n",
    "        pydirectinput.press('enter')\n",
    "\n",
    "        # Reset variables\n",
    "        self.previous_reward = 0\n",
    "        self.steps = 0\n",
    "        self.cur_held_buttons = {'a': False, 's': False, 'w': False, 'd': False}\n",
    "\n",
    "        # Take screenshot to pass to observation\n",
    "        screenshot = np.array(self.capture.grab(self.window_location))[:,:,:-1].astype(np.uint8)\n",
    "        downscaled_screenshot = self.screenshot_helper.downscaleImage(screenshot, (225, 150), (1, 150, 225))\n",
    "        \n",
    "        return self.get_observation(downscaled_screenshot)\n",
    "\n",
    "    # Returns image of gameplay\n",
    "    def get_observation(self, img):\n",
    "\n",
    "        # Crop gameplay part of the window screenshot\n",
    "        obs = img[:, self.game_location['top']:(self.game_location['top'] + self.game_location['height']), self.game_location['left']:(self.game_location['left'] + self.game_location['width'])]\n",
    "\n",
    "        return obs\n",
    "\n",
    "    # Returns the current score as a reward\n",
    "    def get_reward(self, img, action):\n",
    "\n",
    "        # Crop score part of the window screenshot and top of the gameplay section\n",
    "        score_img = img[self.score_location['top']:(self.score_location['top'] + self.score_location['height']), self.score_location['left']:(self.score_location['left'] + self.score_location['width'])]\n",
    "        past_arrows_img = img[env.past_arrows_location['top']:(env.past_arrows_location['top'] + env.past_arrows_location['height']), env.past_arrows_location['left']:(env.past_arrows_location['left'] + env.past_arrows_location['width'])]\n",
    "\n",
    "        # If no input should have occured, give negative reward\n",
    "        if (self.pattern_recog_helper.input_expected(past_arrows_img, action) == False):\n",
    "            return -1\n",
    "\n",
    "        # If the score increased, give positive reward\n",
    "        new_reward = self.pattern_recog_helper.analyze_score(score_img)\n",
    "        if (new_reward > self.previous_reward):\n",
    "            # Set the current reward as the previous reward for the next iteration\n",
    "            self.previous_reward = new_reward\n",
    "            return 1\n",
    "            \n",
    "        # If the score didn't change, and no action has taken place, give a neutral reward\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Checks if the game is over\n",
    "    def get_over(self, img):\n",
    "\n",
    "        # Crop done part of the window screenshot\n",
    "        obs = img[self.done_location['top']:(self.done_location['top'] + self.done_location['height']), self.done_location['left']:(self.done_location['left'] + self.done_location['width'])]\n",
    "        \n",
    "        return self.pattern_recog_helper.analyze_results(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment started - Using random inputs')\n",
    "time.sleep(2)\n",
    "for episode in range(10):\n",
    "  done = False\n",
    "  start = time.perf_counter()\n",
    "  total_reward = 0\n",
    "  while not done:\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    total_reward += reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "  print(f'Total Reward for episode {episode} is {total_reward}')\n",
    "  print(f'Total steps during this episode: {env.steps}')\n",
    "  print(f'Total duration of this episode is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {env.steps / final_time} steps per second')\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the environment is valid\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function, that is called after every step\n",
    "# This is used to save the model in regular intervals\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "  def __init__(self, checking_freq, save_path, verbose=1):\n",
    "    super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "    self.checking_freq = checking_freq\n",
    "    self.save_path = save_path\n",
    "\n",
    "  def _init_callback(self):\n",
    "    if self.save_path is not None:\n",
    "      os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "  def _on_step(self):\n",
    "    if (self.num_timesteps % 500 == 0):\n",
    "      self.logger.dump(self.num_timesteps)\n",
    "    if self.n_calls % self.checking_freq == 0:\n",
    "      model_path = os.path.join(self.save_path, f'best_model_{self.n_calls}')\n",
    "      self.model.save(model_path)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './training/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(checking_freq=1_000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StepManiaEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap environment to monitor performance and training process\n",
    "env = gym.make(\"StepManiaEnv-v1\")\n",
    "env = Monitor(env, filename=\"./logs/stepmania-env-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "  'CnnPolicy',              \n",
    "  env,                      # Used environment\n",
    "  tensorboard_log=LOG_DIR,  # Log directory\n",
    "  verbose=1,                # Enables logging\n",
    "  learning_rate=0.0005,     # Learning rate of the optimizer used in training\n",
    "  buffer_size=120_000,      # Buffer size depending on amount of ram\n",
    "  learning_starts=1_000,    # Learning starts after 1000 steps\n",
    "  # device='cpu'              # Training on cpu or gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "model.learn(total_timesteps=20_000, callback=callback, log_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x20091dd7c10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load past model \n",
    "model.load(r'training\\best_model_10000_DQN3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment started - Using Model')\n",
    "for episode in range(10):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  final_reward = 0\n",
    "  start = time.perf_counter()\n",
    "  while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(int(action))\n",
    "    final_reward += reward\n",
    "\n",
    "  stop = time.perf_counter()\n",
    "  final_time = stop - start\n",
    "\n",
    "  print(f'Total Reward for episode {episode} is {env.previous_reward}')\n",
    "  print(f'Total steps during this episode: {env.steps}')\n",
    "  print(f'Total duration of this episode is {final_time:0.4f} seconds')\n",
    "  print(f'This equals an average of {env.steps / final_time} steps per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "results_plotter.plot_results(\n",
    "  [LOG_DIR], 10000, results_plotter.X_TIMESTEPS, \"StepManiaEnv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
